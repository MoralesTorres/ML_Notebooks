{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Copia 2 de TextgGenerationLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM05P/uJthEXGVRx6eO/Dpc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoralesTorres/ML_Notebooks/blob/master/CodigoFinalUPYlstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfdDuCiMgT8q"
      },
      "source": [
        "1. IMPORTING DEPENDENCIES\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTZDVs0Tjcyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e014633-635c-491a-841c-598f0e51a66c"
      },
      "source": [
        "!pip install -q gradio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 22.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 22.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 962kB 56.8MB/s \n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for Flask-BasicAuth (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flask-cachebuster (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1jwzAFAqxjT"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf \n",
        "import string \n",
        "import requests \n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U7deAQhrRG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6987e751-fb90-44d7-ee77-5250b099d121"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW8cmpCErqmt"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/CORPUSex.txt\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    advanced = f.read()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ptCEhpnJOF"
      },
      "source": [
        "At this point, we set all data lines to create a long string consisting of the data in a continuous format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XyggXHhLr8yY",
        "outputId": "e215a11f-4b48-43b7-de05-d32b2c9f8f4e"
      },
      "source": [
        "data = advanced.split('\\n')\n",
        "data[0]\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv7JcfSMtBZx"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "LgPUTQagtMp7",
        "outputId": "cc44077f-9b18-4f95-ce81-e964c86e62ae"
      },
      "source": [
        "data[3]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"In the afternoon, I go to my English class. That's from five to six. After that, I go shopping for food and things for the house. Then I often listen to my audio CD. And I listen and repeat! Sometimes I do my homework. At about half past eight I cook dinner for my family and me. We usually have dinner at nine thirty. Then we watch TV or read. I often go to bed at about half past eleven.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65T-y30XtPx7",
        "outputId": "7902141e-5f2b-4c6d-983a-f0b20b07052f"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1168"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYwQAkleteV0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "6ed14bed-c6c0-4908-a3fb-43bcfcb6ef73"
      },
      "source": [
        "data = \" \".join(data)\n",
        "data[:1000]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" On weekdays except Friday (this is, from Monday to Thursday) I get up early – at half past seven. I have a white coffee and biscuits for breakfast and I go to work. I work from eight to five. I always have lunch at the office. I usually have a sandwich and a juice or a piece of fruit for lunch.  In the afternoon, I go to my English class. That's from five to six. After that, I go shopping for food and things for the house. Then I often listen to my audio CD. And I listen and repeat! Sometimes I do my homework. At about half past eight I cook dinner for my family and me. We usually have dinner at nine thirty. Then we watch TV or read. I often go to bed at about half past eleven. On Friday I work from nine to one. Then I have lunch and at four I go to my course. I'm taking a course on the Internet. It's interesting. I finish at six. Then I go out for a drink with my friends. At the weekend my day is different. On Saturday, in the morning, oh well, on Saturday morning my family and I do \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu6HjNgqpiS6"
      },
      "source": [
        "Creation of a function to remove all the punctuation marks and special characters from the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SwqFn-uuBUT"
      },
      "source": [
        "def clean_text(doc):\n",
        "  tokens = doc.split()\n",
        "  table = str.maketrans('' , '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88LVOAmIvXS-",
        "outputId": "e6bf7dd2-cb31-4f00-ceb6-0a122f375f8d"
      },
      "source": [
        "tokens = clean_text(data)\n",
        "print(tokens[:50])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['on', 'weekdays', 'except', 'friday', 'this', 'is', 'from', 'monday', 'to', 'thursday', 'i', 'get', 'up', 'early', 'at', 'half', 'past', 'seven', 'i', 'have', 'a', 'white', 'coffee', 'and', 'biscuits', 'for', 'breakfast', 'and', 'i', 'go', 'to', 'work', 'i', 'work', 'from', 'eight', 'to', 'five', 'i', 'always', 'have', 'lunch', 'at', 'the', 'office', 'i', 'usually', 'have', 'a', 'sandwich']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGpSCE5rwet5",
        "outputId": "eba7dcf2-b33f-45dc-c4a0-db9943683c40"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq742MlfwjXO",
        "outputId": "907f9df0-5779-419d-e7d5-ac9b1e595da8"
      },
      "source": [
        "len(set(tokens))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUvxw-S9p8jQ"
      },
      "source": [
        "TOKENIZATION PROCESS\n",
        "---cd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhskkOu6qKBA"
      },
      "source": [
        "**Encoder-Decoder model with ATTENTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRCf_l9ehLTp",
        "outputId": "8b10cbe1-d3f1-484a-c49f-a1392f7b13e1"
      },
      "source": [
        "\n",
        "from random import randint\n",
        "\n",
        "def generate_sequence(length, n_unique):\n",
        "  return [randint(0, n_unique-1) for _ in range(length)]\n",
        "\n",
        "sequence = generate_sequence(10, 50)\n",
        "print(sequence)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15, 6, 36, 14, 47, 29, 49, 46, 28, 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGklBHb8hiY3"
      },
      "source": [
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-3hyvNGhrjw"
      },
      "source": [
        "\n",
        "\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voG9_frMh0bv",
        "outputId": "d1cb0474-7ff8-40f6-fff7-952389645176"
      },
      "source": [
        "\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "\n",
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        "\n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        "\n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        "\n",
        "# generate random sequence\n",
        "sequence = generate_sequence(5, 50)\n",
        "print(sequence)\n",
        "# one hot encode\n",
        "encoded = one_hot_encode(sequence, 50)\n",
        "print(encoded)\n",
        "# decode\n",
        "decoded = one_hot_decode(encoded)\n",
        "print(decoded)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36, 22, 32, 47, 34]\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "[36, 22, 32, 47, 34]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoAJIGM_iDVo"
      },
      "source": [
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, n_unique):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, n_unique)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, n_unique)\n",
        "\ty = one_hot_encode(sequence_out, n_unique)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGSg_mCeiElP",
        "outputId": "6d5720c5-8faf-498e-c7b8-6da3929f4ca0"
      },
      "source": [
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        " \n",
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        " \n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        " \n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        " \n",
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, n_unique):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, n_unique)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, n_unique)\n",
        "\ty = one_hot_encode(sequence_out, n_unique)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y\n",
        " \n",
        "# generate random sequence\n",
        "X, y = get_pair(5, 2, 50)\n",
        "print(X.shape, y.shape)\n",
        "print('X=%s, y=%s' % (one_hot_decode(X[0]), one_hot_decode(y[0])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 5, 50) (1, 5, 50)\n",
            "X=[43, 25, 32, 14, 41], y=[43, 25, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAB8w5G4N5Bu"
      },
      "source": [
        "At this point, the Encoder-Decor generate pairs. It is seen the first two integers of the sequence followed by a padding of zero values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVckoHY6OYBY"
      },
      "source": [
        "from keras import initializers"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOq0bng1iEoX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "\n",
        "from keras.engine import InputSpec\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frJE3Fg9AELe"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_tOEom2O50d",
        "outputId": "e6796295-8edf-4c15-e613-06773c132c03"
      },
      "source": [
        "pip install attention"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting attention\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/3f/d8b19195a2f5827dcbf0ee6d7e6fe4352f42dcc60693bdb1e431440c8b59/attention-4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow>=2.1 in /usr/local/lib/python3.7/dist-packages (from attention) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from attention) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.12)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.4.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.12.4)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.2.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.1.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (56.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.28.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->attention) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->attention) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (1.24.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->attention) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->attention) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (0.4.8)\n",
            "Installing collected packages: attention\n",
            "Successfully installed attention-4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ScMckcYO56Z",
        "outputId": "df39805b-3e65-42c5-8f8a-694942d5a8b8"
      },
      "source": [
        "pip install keras-self-attention"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/34/e21dc6adcdab2be03781bde78c6c5d2b2136d35a1dd3e692d7e160ba062a/keras-self-attention-0.49.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-self-attention) (1.15.0)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.49.0-cp37-none-any.whl size=19468 sha256=52f7f804d2a5cd6d6dcc495402d4af08f5d5dbce65ab1839e6c024a5b13130d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/9d/c5/26693a5092d9313daeae94db04818fc0a2b7a48ea381989f34\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.49.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4fMUpYLPHjh",
        "outputId": "c65c8d1d-188e-463a-a3bb-44af4177c566"
      },
      "source": [
        "pip install phased-lstm-keras"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting phased-lstm-keras\n",
            "  Downloading https://files.pythonhosted.org/packages/73/07/20fe5a59c4e11a42c17763219eb5a3de7c827be7b7d9c1afa5834a4e19b1/phased_lstm_keras-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from phased-lstm-keras) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from phased-lstm-keras) (1.19.5)\n",
            "Requirement already satisfied: keras>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from phased-lstm-keras) (2.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->phased-lstm-keras) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->phased-lstm-keras) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->phased-lstm-keras) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->phased-lstm-keras) (0.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.2->phased-lstm-keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.2->phased-lstm-keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.2->phased-lstm-keras) (2.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0.0->phased-lstm-keras) (1.15.0)\n",
            "Installing collected packages: phased-lstm-keras\n",
            "Successfully installed phased-lstm-keras-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHbsanziPH5p"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM\n",
        "from keras.layers.wrappers import TimeDistributed"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZzqeDncPdTD"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-dGnE6aPdbs"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKB7bLyfOKtO"
      },
      "source": [
        "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
        " \n",
        "class AttentionDecoder(LSTM):\n",
        " \n",
        "    def __init__(self, units, output_dim,\n",
        "                 activation='tanh',\n",
        "                 return_probabilities=False,\n",
        "                 name='AttentionDecoder',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
        "        encoder and outputs the decoded states\n",
        "        :param units: dimension of the hidden state and the attention matrices\n",
        "        :param output_dim: the number of labels in the output space\n",
        " \n",
        "        references:\n",
        "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
        "            \"Neural machine translation by jointly learning to align and translate.\"\n",
        "            arXiv preprint arXiv:1409.0473 (2014).\n",
        "        \"\"\"\n",
        "        self.units = units\n",
        "        self.output_dim = output_dim\n",
        "        self.return_probabilities = return_probabilities\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        " \n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        " \n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        " \n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\n",
        "        self.name = name\n",
        "        self.return_sequences = True  # must return sequences\n",
        " \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
        "          for model details that correspond to the matrices here.\n",
        "        \"\"\"\n",
        " \n",
        "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
        " \n",
        "        if self.stateful:\n",
        "            super(AttentionDecoder, self).reset_states()\n",
        " \n",
        "        self.states = [None, None]  # y, s\n",
        " \n",
        "        \"\"\"\n",
        "            Matrices for creating the context vector\n",
        "        \"\"\"\n",
        " \n",
        "        self.V_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='V_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='W_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='U_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.b_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_a',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the r (reset) gate\n",
        "        \"\"\"\n",
        "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_r = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_r',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        " \n",
        "        \"\"\"\n",
        "            Matrices for the z (update) gate\n",
        "        \"\"\"\n",
        "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_z = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_z',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the proposal\n",
        "        \"\"\"\n",
        "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_p = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_p',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for making the final prediction vector\n",
        "        \"\"\"\n",
        "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                   name='C_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
        "                                   name='U_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
        "                                   name='W_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
        "                                   name='b_o',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        " \n",
        "        # For creating the initial state:\n",
        "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='W_s',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        " \n",
        "        self.input_spec = [\n",
        "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
        "        self.built = True\n",
        " \n",
        "    def call(self, x):\n",
        "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
        "        self.x_seq = x\n",
        " \n",
        "        # apply the a dense layer over the time dimension of the sequence\n",
        "        # do it here because it doesn't depend on any previous steps\n",
        "        # thefore we can save computation time:\n",
        "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
        "                                             input_dim=self.input_dim,\n",
        "                                             timesteps=self.timesteps,\n",
        "                                             output_dim=self.units)\n",
        " \n",
        "        return super(AttentionDecoder, self).call(x)\n",
        " \n",
        "    def get_initial_state(self, inputs):\n",
        "        # apply the matrix on the first time step to get the initial s0.\n",
        "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
        " \n",
        "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
        "        # output_dim)\n",
        "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
        "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
        "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
        "        y0 = K.tile(y0, [1, self.output_dim])\n",
        " \n",
        "        return [y0, s0]\n",
        " \n",
        "    def step(self, x, states):\n",
        " \n",
        "        ytm, stm = states\n",
        " \n",
        "        # repeat the hidden state to the length of the sequence\n",
        "        _stm = K.repeat(stm, self.timesteps)\n",
        " \n",
        "        # now multiplty the weight matrix with the repeated hidden state\n",
        "        _Wxstm = K.dot(_stm, self.W_a)\n",
        " \n",
        "        # calculate the attention probabilities\n",
        "        # this relates how much other timesteps contributed to this one.\n",
        "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
        "                   K.expand_dims(self.V_a))\n",
        "        at = K.exp(et)\n",
        "        at_sum = K.sum(at, axis=1)\n",
        "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
        "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
        " \n",
        "        # calculate the context vector\n",
        "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
        "        # ~~~> calculate new hidden state\n",
        "        # first calculate the \"r\" gate:\n",
        " \n",
        "        rt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_r)\n",
        "            + K.dot(stm, self.U_r)\n",
        "            + K.dot(context, self.C_r)\n",
        "            + self.b_r)\n",
        " \n",
        "        # now calculate the \"z\" gate\n",
        "        zt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_z)\n",
        "            + K.dot(stm, self.U_z)\n",
        "            + K.dot(context, self.C_z)\n",
        "            + self.b_z)\n",
        " \n",
        "        # calculate the proposal hidden state:\n",
        "        s_tp = activations.tanh(\n",
        "            K.dot(ytm, self.W_p)\n",
        "            + K.dot((rt * stm), self.U_p)\n",
        "            + K.dot(context, self.C_p)\n",
        "            + self.b_p)\n",
        " \n",
        "        # new hidden state:\n",
        "        st = (1-zt)*stm + zt * s_tp\n",
        " \n",
        "        yt = activations.softmax(\n",
        "            K.dot(ytm, self.W_o)\n",
        "            + K.dot(stm, self.U_o)\n",
        "            + K.dot(context, self.C_o)\n",
        "            + self.b_o)\n",
        " \n",
        "        if self.return_probabilities:\n",
        "            return at, [yt, st]\n",
        "        else:\n",
        "            return yt, [yt, st]\n",
        " \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "            For Keras internal compatability checking\n",
        "        \"\"\"\n",
        "        if self.return_probabilities:\n",
        "            return (None, self.timesteps, self.timesteps)\n",
        "        else:\n",
        "            return (None, self.timesteps, self.output_dim)\n",
        " \n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'units': self.units,\n",
        "            'return_probabilities': self.return_probabilities\n",
        "        }\n",
        "        base_config = super(AttentionDecoder, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV_-E7aAOKwz"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIDNhnttOKzl"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBltTNR4iErS"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKMMC7-8iEvS"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtUc7dZPxAMI",
        "outputId": "d05c74a9-f9bb-45e9-db37-021125b407c6"
      },
      "source": [
        "length = 10 + 1\n",
        "lines = []\n",
        "\n",
        "for i in range(length, len(tokens)):\n",
        "  seq = tokens[i-length:i]\n",
        "  line = ' '.join(seq)\n",
        "  lines.append(line)\n",
        "  if i > 50000:\n",
        "    break\n",
        "\n",
        "print(len(lines))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6gfOaLjqZuaU",
        "outputId": "8b4b6b7b-3fd4-472b-a99e-f8729d390b72"
      },
      "source": [
        "lines[2]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'except friday this is from monday to thursday i get up'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnNM9qroyyJ5"
      },
      "source": [
        "## Construction of our LSTM model / Training X and Y\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "McTn9gREyY-P",
        "outputId": "448e1de1-6394-42a9-ae4b-042af97bf868"
      },
      "source": [
        "tokens[30]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'to'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeP4la-izKNT"
      },
      "source": [
        "import numpy as np \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzYUYwT6zwzQ"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybuHI89A0PpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7433a17-4f98-4136-9ced-28ca1bc3e76a"
      },
      "source": [
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:,-1]\n",
        "X[0]\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9, 93, 92, 31, 90, 33, 13, 87,  3, 86])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UxRoMta1RcS",
        "outputId": "66691412-2d8e-4f20-f34e-a2106cf334b2"
      },
      "source": [
        "X[1]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([93, 92, 31, 90, 33, 13, 87,  3, 86,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXp1nRvz12Ff"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rUoqRQi1ZZS",
        "outputId": "d52a10f7-e451-4253-c693-76d30666b5b8"
      },
      "source": [
        "y[1]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7P79ceW1m2I",
        "outputId": "716690bd-81e0-4550-a4d1-729c9af34d17"
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size )\n",
        "X.shape[1]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUJAkcAz20hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2250ec-3f1f-458d-e9f7-ccc58f0da960"
      },
      "source": [
        "seq_length = X.shape[1]\n",
        "seq_length\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohv0WKTNH89H"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W17FVAMlH9Am"
      },
      "source": [
        "from keras.layers import Embedding"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HP4X71EW1tr"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQybdIk6AcZ4",
        "outputId": "3d55d41f-aeca-41d2-e0dc-a99287424f04"
      },
      "source": [
        "pip install attention"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: attention in /usr/local/lib/python3.7/dist-packages (4.0)\n",
            "Requirement already satisfied: tensorflow>=2.1 in /usr/local/lib/python3.7/dist-packages (from attention) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from attention) (1.19.5)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.4.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.3.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.36.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.15.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.32.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.28.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (56.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (0.4.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (4.7.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->attention) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->attention) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->attention) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->attention) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DQuwkKLAhT8",
        "outputId": "abb7c0ae-6f66-4eab-e973-807fe35e2f43"
      },
      "source": [
        "pip install keras-self-attention"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.7/dist-packages (0.49.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-self-attention) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LPb2ADkAchW"
      },
      "source": [
        "#variational LSTM dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK7o6QKa3B3T"
      },
      "source": [
        "## LSTM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKLh4covy6qA"
      },
      "source": [
        "from __future__ import print_function\n",
        "#import Keras library\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM, Input, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.metrics import categorical_accuracy\n",
        "\n",
        "#import spacy, and spacy french model\n",
        "# spacy is used to work on text\n",
        "\n",
        "\n",
        "#import other libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import collections\n",
        "from six.moves import cPickle"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGYlbBR_8Cxr"
      },
      "source": [
        "import sys, os, re, csv, codecs, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aQUetEs8TM8"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jW2owFh8TQV"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-WVTNJoxwU-",
        "outputId": "3ac4440f-dcf4-4d88-d0b1-8a39151ed787"
      },
      "source": [
        "X, X_test, y, Y_test = train_test_split(X, y, test_size = 0.5)\n",
        "print(X.shape, y.shape, X_test.shape, Y_test.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18691, 10) (18691, 125) (18692, 10) (18692, 125)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFRrfpFzzjt9"
      },
      "source": [
        "\n",
        "# fit an LSTM network to training data\n",
        "def fit_lstm(train, n_batch, nb_epoch, n_neurons, dropout):\n",
        "\tX, y = train[:, 0:-1], train[:, -1]\n",
        "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(LSTM(n_neurons=vocab_size, seq_length=(n_batch, X.shape[1], X.shape[2]), stateful=True, recurrent_dropout=dropout))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\tfor i in range(nb_epoch):\n",
        "\t\tmodel.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
        "\t\tmodel.reset_states()\n",
        "\treturn model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3RLhP_4zj4v"
      },
      "source": [
        "# run a repeated experiment\n",
        "def experiment(series, n_lag, n_repeats, n_epochs, n_batch, n_neurons, dropout):\n",
        "\t# transform data to be stationary\n",
        "\traw_values = series.values\n",
        "\tdiff_values = difference(raw_values, 1)\n",
        "\t# transform data to be supervised learning\n",
        "\tsupervised = timeseries_to_supervised(diff_values, n_lag)\n",
        "\tsupervised_values = supervised.values[n_lag:,:]\n",
        "\t# split data into train and test-sets\n",
        "\ttrain, test = supervised_values[0:-12], supervised_values[-12:]\n",
        "\t# transform the scale of the data\n",
        "\tscaler, train_scaled, test_scaled = scale(train, test)\n",
        "\t# run experiment\n",
        "\terror_scores = list()\n",
        "\tfor r in range(n_repeats):\n",
        "\t\t# fit the model\n",
        "\t\ttrain_trimmed = train_scaled[2:, :]\n",
        "\t\tlstm_model = fit_lstm(train_trimmed, n_batch, n_epochs, n_neurons, dropout)\n",
        "\t\t# forecast test dataset\n",
        "\t\ttest_reshaped = test_scaled[:,0:-1]\n",
        "\t\ttest_reshaped = test_reshaped.reshape(len(test_reshaped), 1, 1)\n",
        "\t\toutput = lstm_model.predict(test_reshaped, batch_size=n_batch)\n",
        "\t\tpredictions = list()\n",
        "\t\tfor i in range(len(output)):\n",
        "\t\t\tyhat = output[i,0]\n",
        "\t\t\tX = test_scaled[i, 0:-1]\n",
        "\t\t\t# invert scaling\n",
        "\t\t\tyhat = invert_scale(scaler, X, yhat)\n",
        "\t\t\t# invert differencing\n",
        "\t\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
        "\t\t\t# store forecast\n",
        "\t\t\tpredictions.append(yhat)\n",
        "\t\t# report performance\n",
        "\t\trmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
        "\t\tprint('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
        "\t\terror_scores.append(rmse)\n",
        "\treturn error_scores"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wJwyqV_zj71"
      },
      "source": [
        "# configure the experiment\n",
        "def run():\n",
        "\t# load dataset\n",
        "\tseries = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
        "\t# configure the experiment\n",
        "\tn_lag = 1\n",
        "\tn_repeats = 30\n",
        "\tn_epochs = 1000\n",
        "\tn_batch = 4\n",
        "\tn_neurons = 3\n",
        "\tn_dropout = [0.0, 0.2, 0.4, 0.6]\n",
        "\t# run the experiment\n",
        "\tresults = DataFrame()\n",
        "\tfor dropout in n_dropout:\n",
        "\t\tresults[str(dropout)] = experiment(series, n_lag, n_repeats, n_epochs, n_batch, n_neurons, dropout)\n",
        "\t# summarize results\n",
        "\tprint(results.describe())\n",
        "\t# save boxplot\n",
        "\tresults.boxplot()\n",
        "\tpyplot.savefig('experiment_dropout_recurrent.png')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wNlAhPlxwXp"
      },
      "source": [
        "# fit an LSTM network to training data\n",
        "def fit_lstm(train, test, raw, scaler, batch_size, nb_epoch, neurons, dropout):\n",
        "\tX, y = train[:, 0:-1], train[:, -1]\n",
        "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\t# prepare model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True, recurrent_dropout=dropout))\n",
        "\tmodel.add(Dense(1))\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\t# fit model\n",
        "\ttrain_rmse, test_rmse = list(), list()\n",
        "\tfor i in range(nb_epoch):\n",
        "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
        "\t\tmodel.reset_states()\n",
        "\t\t# evaluate model on train data\n",
        "\t\traw_train = raw[-(len(train)+len(test)+1):-len(test)]\n",
        "\t\ttrain_rmse.append(evaluate(model, raw_train, train, scaler, 0, batch_size))\n",
        "\t\t# evaluate model on test data\n",
        "\t\traw_test = raw[-(len(test)+1):]\n",
        "\t\ttest_rmse.append(evaluate(model, raw_test, test, scaler, 0, batch_size))\n",
        "\thistory = DataFrame()\n",
        "\thistory['train'], history['test'] = train_rmse, test_rmse\n",
        "\treturn history\n",
        " \n",
        "# run diagnostic experiments\n",
        "def run():\n",
        "\t# config\n",
        "\tn_lag = 1\n",
        "\tn_repeats = 10\n",
        "\tn_epochs = 1000\n",
        "\tn_batch = 4\n",
        "\tn_neurons = 3\n",
        "\tdropout = 0.4\n",
        "\t# load dataset\n",
        "\tseries = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
        "\t# transform data to be stationary\n",
        "\traw_values = series.values\n",
        "\tdiff_values = difference(raw_values, 1)\n",
        "\t# transform data to be supervised learning\n",
        "\tsupervised = timeseries_to_supervised(diff_values, n_lag)\n",
        "\tsupervised_values = supervised.values[n_lag:,:]\n",
        "\t# split data into train and test-sets\n",
        "\ttrain, test = supervised_values[0:-12], supervised_values[-12:]\n",
        "\t# transform the scale of the data\n",
        "\tscaler, train_scaled, test_scaled = scale(train, test)\n",
        "\t# fit and evaluate model\n",
        "\ttrain_trimmed = train_scaled[2:, :]\n",
        "\t# run diagnostic tests\n",
        "\tfor i in range(n_repeats):\n",
        "\t\thistory = fit_lstm(train_trimmed, test_scaled, raw_values, scaler, n_batch, n_epochs, n_neurons, dropout)\n",
        "\t\tpyplot.plot(history['train'], color='blue')\n",
        "\t\tpyplot.plot(history['test'], color='orange')\n",
        "\t\tprint('%d) TrainRMSE=%f, TestRMSE=%f' % (i+1, history['train'].iloc[-1], history['test'].iloc[-1]))\n",
        "\tpyplot.savefig('diagnostic_dropout_recurrent.png')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deoR27I22T1L"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u7whT0U2T5D"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gujuM_3b2T9V"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbzx1RRWjKsc"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 70, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8xvx_HZxrNi"
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9OoU4L5ialN"
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfhKcEdxZDSz",
        "outputId": "6465f553-5100-4e57-e669-7d705b5be6f2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 10, 70)            8750      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 10, 100)           68400     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 125)               12625     \n",
            "=================================================================\n",
            "Total params: 180,275\n",
            "Trainable params: 180,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfqh_7Qt3Wsn"
      },
      "source": [
        ""
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8peP4AohkTvw"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1LzgsrL3WxI"
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nPeCet6kTzQ",
        "outputId": "e89f77d8-ecab-49fd-f536-ed83494ebe54"
      },
      "source": [
        "model.fit(X, y, batch_size  = 100, epochs = 50)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "187/187 [==============================] - 35s 7ms/step - loss: 4.3322 - accuracy: 0.0801\n",
            "Epoch 2/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 2.5685 - accuracy: 0.2799\n",
            "Epoch 3/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 1.0026 - accuracy: 0.7408\n",
            "Epoch 4/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.4380 - accuracy: 0.8996\n",
            "Epoch 5/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.2269 - accuracy: 0.9487\n",
            "Epoch 6/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.1368 - accuracy: 0.9750\n",
            "Epoch 7/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.0797 - accuracy: 0.9891\n",
            "Epoch 8/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0420 - accuracy: 0.9945\n",
            "Epoch 9/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.0329 - accuracy: 0.9960\n",
            "Epoch 10/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.3528 - accuracy: 0.9044\n",
            "Epoch 11/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0326 - accuracy: 0.9971\n",
            "Epoch 12/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0181 - accuracy: 0.9985\n",
            "Epoch 13/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0125 - accuracy: 0.9989\n",
            "Epoch 14/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.0115 - accuracy: 0.9988\n",
            "Epoch 15/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0181 - accuracy: 0.9974\n",
            "Epoch 16/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.1010 - accuracy: 0.9757\n",
            "Epoch 17/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0093 - accuracy: 0.9991\n",
            "Epoch 18/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0098 - accuracy: 0.9989\n",
            "Epoch 19/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0103 - accuracy: 0.9988\n",
            "Epoch 20/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0064 - accuracy: 0.9992\n",
            "Epoch 21/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0065 - accuracy: 0.9991\n",
            "Epoch 22/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0262 - accuracy: 0.9936\n",
            "Epoch 23/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0618 - accuracy: 0.9840\n",
            "Epoch 24/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0071 - accuracy: 0.9995\n",
            "Epoch 25/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0050 - accuracy: 0.9995\n",
            "Epoch 26/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0084 - accuracy: 0.9988\n",
            "Epoch 27/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.0070 - accuracy: 0.9990\n",
            "Epoch 28/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9996\n",
            "Epoch 29/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0681 - accuracy: 0.9822\n",
            "Epoch 30/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0078 - accuracy: 0.9993\n",
            "Epoch 31/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9993\n",
            "Epoch 32/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0061 - accuracy: 0.9992\n",
            "Epoch 33/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0100 - accuracy: 0.9983\n",
            "Epoch 34/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0058 - accuracy: 0.9990\n",
            "Epoch 35/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9995\n",
            "Epoch 36/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0047 - accuracy: 0.9994\n",
            "Epoch 37/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.0051 - accuracy: 0.9993\n",
            "Epoch 38/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0053 - accuracy: 0.9992\n",
            "Epoch 39/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0051 - accuracy: 0.9992\n",
            "Epoch 40/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.2568 - accuracy: 0.9346\n",
            "Epoch 41/50\n",
            "187/187 [==============================] - 1s 6ms/step - loss: 0.0109 - accuracy: 0.9983\n",
            "Epoch 42/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9995\n",
            "Epoch 43/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0053 - accuracy: 0.9991\n",
            "Epoch 44/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0056 - accuracy: 0.9991\n",
            "Epoch 45/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0044 - accuracy: 0.9994\n",
            "Epoch 46/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9995\n",
            "Epoch 47/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 0.9995\n",
            "Epoch 48/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9994\n",
            "Epoch 49/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9998\n",
            "Epoch 50/50\n",
            "187/187 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd4f037c510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0RinKKVkT2Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDffJ3MlswC"
      },
      "source": [
        "seed_text=lines[150]\n",
        "seed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbZ_Cg_1lszx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxQNVl-NKElr"
      },
      "source": [
        "\n",
        "import sys, os, re, csv, codecs, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2JECigBSll3"
      },
      "source": [
        "\n",
        "batch_size = 100\n",
        "epochs = 2\n",
        "model.fit(X,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcp3-fbzKHJD"
      },
      "source": [
        "\n",
        "history = model.history\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "# Get number of epochs\\n\",\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\\n\",\n",
        "plt.plot(epochs, acc)\n",
        "plt.plot(epochs, val_acc)\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure() \n",
        "\n",
        "# Plot training and validation loss per epoch\\n\",\n",
        "plt.plot(epochs, loss)\n",
        "plt.plot(epochs, val_loss)\n",
        "plt.title('Training and validation loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD2wqV2sNCBm"
      },
      "source": [
        "model.evaluate(X_test, Y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3gEz80lXltJ"
      },
      "source": [
        "seed_text=lines[0]\n",
        "seed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgqu8cYpO--z"
      },
      "source": [
        "\n",
        "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
        "  text = []\n",
        "\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
        "\n",
        "    y_predict = model.predict_classes(encoded)\n",
        "\n",
        "    predicted_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == y_predict:\n",
        "        predicted_word = word\n",
        "        break\n",
        "    seed_text = seed_text + ' ' + predicted_word\n",
        "    text.append(predicted_word)\n",
        "  return ' '.join(text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uXN43vqNUHs"
      },
      "source": [
        "\n",
        "generate_text_seq(model, tokenizer, seq_length, seed_text, 15) \n",
        "\n",
        " \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jd-CagnQkMA"
      },
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "def prediction_text(seed_text,n_words):    \n",
        " text_seq_length = 3\n",
        " # seed_text =             \"this is the\" ...\n",
        " output = generate_text_seq(model, tokenizer, text_seq_length, seed_text, int(n_words)) \n",
        " return output\n",
        "\n",
        "\n",
        "iface = gr.Interface(fn=prediction_text, inputs= [\"text\", \"text\"], outputs=\"text\")\n",
        "iface.launch()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}