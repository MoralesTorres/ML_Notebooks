{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Copia 2 de TextgGenerationLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkJhb1z1rC4uHjmcejvby/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoralesTorres/ML_Notebooks/blob/master/ModelLSTMwithEncoder-DecoderAttentionUPYborrador.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfdDuCiMgT8q"
      },
      "source": [
        "1. IMPORTING DEPENDENCIES\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTZDVs0Tjcyr"
      },
      "source": [
        "!pip install -q gradio"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1jwzAFAqxjT"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf \n",
        "import string \n",
        "import requests \n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U7deAQhrRG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0cb4bed-c8fd-4e62-c4c3-fd4bdc383850"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW8cmpCErqmt"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/basic.txt\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    advanced = f.read()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "XyggXHhLr8yY",
        "outputId": "76778e4c-c54f-45ed-a1cb-7e1355b7bdad"
      },
      "source": [
        "data = advanced.split('\\n')\n",
        "data[0]\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'On weekdays except Friday (this is, from Monday to Thursday) I get up early – at half past seven. I have a white coffee and biscuits for breakfast and I go to work. I work from eight to five. I always have lunch at the office. I usually have a sandwich and a juice or a piece of fruit for lunch.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv7JcfSMtBZx"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LgPUTQagtMp7",
        "outputId": "4d55fff1-b474-4f1a-e035-73ef1110dfa8"
      },
      "source": [
        "data[3]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"On Friday I work from nine to one. Then I have lunch and at four I go to my course. I'm taking a course on the Internet. It's interesting. I finish at six. Then I go out for a drink with my friends.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65T-y30XtPx7",
        "outputId": "154fdba7-5bd3-40df-f715-2e414504a90e"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYwQAkleteV0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "cc98eb49-d97f-4eaf-980f-5358509afe59"
      },
      "source": [
        "data = \" \".join(data)\n",
        "data[:1000]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"On weekdays except Friday (this is, from Monday to Thursday) I get up early – at half past seven. I have a white coffee and biscuits for breakfast and I go to work. I work from eight to five. I always have lunch at the office. I usually have a sandwich and a juice or a piece of fruit for lunch.  In the afternoon, I go to my English class. That's from five to six. After that, I go shopping for food and things for the house. Then I often listen to my audio CD. And I listen and repeat! Sometimes I do my homework. At about half past eight I cook dinner for my family and me. We usually have dinner at nine thirty. Then we watch TV or read. I often go to bed at about half past eleven. On Friday I work from nine to one. Then I have lunch and at four I go to my course. I'm taking a course on the Internet. It's interesting. I finish at six. Then I go out for a drink with my friends. At the weekend my day is different. On Saturday, in the morning, oh well, on Saturday morning my family and I do s\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SwqFn-uuBUT"
      },
      "source": [
        "def clean_text(doc):\n",
        "  tokens = doc.split()\n",
        "  table = str.maketrans('' , '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88LVOAmIvXS-",
        "outputId": "ec7f253b-82cd-47f2-9daf-a23538d30cb9"
      },
      "source": [
        "tokens = clean_text(data)\n",
        "print(tokens[:50])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['on', 'weekdays', 'except', 'friday', 'this', 'is', 'from', 'monday', 'to', 'thursday', 'i', 'get', 'up', 'early', 'at', 'half', 'past', 'seven', 'i', 'have', 'a', 'white', 'coffee', 'and', 'biscuits', 'for', 'breakfast', 'and', 'i', 'go', 'to', 'work', 'i', 'work', 'from', 'eight', 'to', 'five', 'i', 'always', 'have', 'lunch', 'at', 'the', 'office', 'i', 'usually', 'have', 'a', 'sandwich']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGpSCE5rwet5",
        "outputId": "2a92e9c1-6dc4-4473-d589-a1517e7d8461"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq742MlfwjXO",
        "outputId": "514fc685-2237-4325-9319-ca00d1326d66"
      },
      "source": [
        "len(set(tokens))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRCf_l9ehLTp",
        "outputId": "13f4ca65-d4fa-45de-aab7-3d420818379a"
      },
      "source": [
        "\n",
        "from random import randint\n",
        "\n",
        "def generate_sequence(length, n_unique):\n",
        "  return [randint(0, n_unique-1) for _ in range(length)]\n",
        "\n",
        "sequence = generate_sequence(10, 50)\n",
        "print(sequence)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16, 49, 49, 31, 49, 44, 44, 45, 16, 46]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGklBHb8hiY3"
      },
      "source": [
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-3hyvNGhrjw"
      },
      "source": [
        "\n",
        "\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voG9_frMh0bv",
        "outputId": "1ed2e62e-7f6f-4ad7-9a79-149ce6572127"
      },
      "source": [
        "\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "\n",
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        "\n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        "\n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        "\n",
        "# generate random sequence\n",
        "sequence = generate_sequence(5, 50)\n",
        "print(sequence)\n",
        "# one hot encode\n",
        "encoded = one_hot_encode(sequence, 50)\n",
        "print(encoded)\n",
        "# decode\n",
        "decoded = one_hot_decode(encoded)\n",
        "print(decoded)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11, 30, 28, 26, 15]\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "[11, 30, 28, 26, 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoAJIGM_iDVo"
      },
      "source": [
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, n_unique):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, n_unique)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, n_unique)\n",
        "\ty = one_hot_encode(sequence_out, n_unique)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGSg_mCeiElP",
        "outputId": "8bb4372e-777d-40ad-cb85-ffca90029c9f"
      },
      "source": [
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        " \n",
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        " \n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        " \n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        " \n",
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, n_unique):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, n_unique)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, n_unique)\n",
        "\ty = one_hot_encode(sequence_out, n_unique)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y\n",
        " \n",
        "# generate random sequence\n",
        "X, y = get_pair(5, 2, 50)\n",
        "print(X.shape, y.shape)\n",
        "print('X=%s, y=%s' % (one_hot_decode(X[0]), one_hot_decode(y[0])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 5, 50) (1, 5, 50)\n",
            "X=[10, 4, 2, 29, 44], y=[10, 4, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAB8w5G4N5Bu"
      },
      "source": [
        "At this point, the Encoder-Decor generate pairs. It is seen the first two integers of the sequence followed by a padding of zero values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVckoHY6OYBY"
      },
      "source": [
        "from keras import initializers"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOq0bng1iEoX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "\n",
        "from keras.engine import InputSpec\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frJE3Fg9AELe"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_tOEom2O50d",
        "outputId": "b9f0fff4-9a42-4c28-a200-7a097e50ded2"
      },
      "source": [
        "pip install attention"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: attention in /usr/local/lib/python3.7/dist-packages (4.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from attention) (1.19.5)\n",
            "Requirement already satisfied: tensorflow>=2.1 in /usr/local/lib/python3.7/dist-packages (from attention) (2.4.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.12.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.10.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.32.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.15.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.3.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.4.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1->attention) (54.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.28.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.8.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->attention) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->attention) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (4.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->attention) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->attention) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ScMckcYO56Z",
        "outputId": "d5c57e38-6822-4b47-ad37-f700c5f2e363"
      },
      "source": [
        "pip install keras-self-attention"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.7/dist-packages (0.49.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-self-attention) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4fMUpYLPHjh",
        "outputId": "e1528e98-9483-43d4-b103-8da54d55e364"
      },
      "source": [
        "pip install phased-lstm-keras"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: phased-lstm-keras in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from phased-lstm-keras) (1.19.5)\n",
            "Requirement already satisfied: keras>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from phased-lstm-keras) (2.4.3)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from phased-lstm-keras) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.2->phased-lstm-keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.2->phased-lstm-keras) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.2->phased-lstm-keras) (3.13)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->phased-lstm-keras) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->phased-lstm-keras) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->phased-lstm-keras) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->phased-lstm-keras) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.2->phased-lstm-keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHbsanziPH5p"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM\n",
        "from keras.layers.wrappers import TimeDistributed"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZzqeDncPdTD"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-dGnE6aPdbs"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKB7bLyfOKtO"
      },
      "source": [
        "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
        " \n",
        "class AttentionDecoder(LSTM):\n",
        " \n",
        "    def __init__(self, units, output_dim,\n",
        "                 activation='tanh',\n",
        "                 return_probabilities=False,\n",
        "                 name='AttentionDecoder',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
        "        encoder and outputs the decoded states\n",
        "        :param units: dimension of the hidden state and the attention matrices\n",
        "        :param output_dim: the number of labels in the output space\n",
        " \n",
        "        references:\n",
        "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
        "            \"Neural machine translation by jointly learning to align and translate.\"\n",
        "            arXiv preprint arXiv:1409.0473 (2014).\n",
        "        \"\"\"\n",
        "        self.units = units\n",
        "        self.output_dim = output_dim\n",
        "        self.return_probabilities = return_probabilities\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        " \n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        " \n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        " \n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\n",
        "        self.name = name\n",
        "        self.return_sequences = True  # must return sequences\n",
        " \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
        "          for model details that correspond to the matrices here.\n",
        "        \"\"\"\n",
        " \n",
        "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
        " \n",
        "        if self.stateful:\n",
        "            super(AttentionDecoder, self).reset_states()\n",
        " \n",
        "        self.states = [None, None]  # y, s\n",
        " \n",
        "        \"\"\"\n",
        "            Matrices for creating the context vector\n",
        "        \"\"\"\n",
        " \n",
        "        self.V_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='V_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='W_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='U_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.b_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_a',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the r (reset) gate\n",
        "        \"\"\"\n",
        "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_r = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_r',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        " \n",
        "        \"\"\"\n",
        "            Matrices for the z (update) gate\n",
        "        \"\"\"\n",
        "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_z = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_z',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the proposal\n",
        "        \"\"\"\n",
        "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_p = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_p',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for making the final prediction vector\n",
        "        \"\"\"\n",
        "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                   name='C_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
        "                                   name='U_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
        "                                   name='W_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
        "                                   name='b_o',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        " \n",
        "        # For creating the initial state:\n",
        "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='W_s',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        " \n",
        "        self.input_spec = [\n",
        "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
        "        self.built = True\n",
        " \n",
        "    def call(self, x):\n",
        "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
        "        self.x_seq = x\n",
        " \n",
        "        # apply the a dense layer over the time dimension of the sequence\n",
        "        # do it here because it doesn't depend on any previous steps\n",
        "        # thefore we can save computation time:\n",
        "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
        "                                             input_dim=self.input_dim,\n",
        "                                             timesteps=self.timesteps,\n",
        "                                             output_dim=self.units)\n",
        " \n",
        "        return super(AttentionDecoder, self).call(x)\n",
        " \n",
        "    def get_initial_state(self, inputs):\n",
        "        # apply the matrix on the first time step to get the initial s0.\n",
        "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
        " \n",
        "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
        "        # output_dim)\n",
        "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
        "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
        "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
        "        y0 = K.tile(y0, [1, self.output_dim])\n",
        " \n",
        "        return [y0, s0]\n",
        " \n",
        "    def step(self, x, states):\n",
        " \n",
        "        ytm, stm = states\n",
        " \n",
        "        # repeat the hidden state to the length of the sequence\n",
        "        _stm = K.repeat(stm, self.timesteps)\n",
        " \n",
        "        # now multiplty the weight matrix with the repeated hidden state\n",
        "        _Wxstm = K.dot(_stm, self.W_a)\n",
        " \n",
        "        # calculate the attention probabilities\n",
        "        # this relates how much other timesteps contributed to this one.\n",
        "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
        "                   K.expand_dims(self.V_a))\n",
        "        at = K.exp(et)\n",
        "        at_sum = K.sum(at, axis=1)\n",
        "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
        "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
        " \n",
        "        # calculate the context vector\n",
        "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
        "        # ~~~> calculate new hidden state\n",
        "        # first calculate the \"r\" gate:\n",
        " \n",
        "        rt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_r)\n",
        "            + K.dot(stm, self.U_r)\n",
        "            + K.dot(context, self.C_r)\n",
        "            + self.b_r)\n",
        " \n",
        "        # now calculate the \"z\" gate\n",
        "        zt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_z)\n",
        "            + K.dot(stm, self.U_z)\n",
        "            + K.dot(context, self.C_z)\n",
        "            + self.b_z)\n",
        " \n",
        "        # calculate the proposal hidden state:\n",
        "        s_tp = activations.tanh(\n",
        "            K.dot(ytm, self.W_p)\n",
        "            + K.dot((rt * stm), self.U_p)\n",
        "            + K.dot(context, self.C_p)\n",
        "            + self.b_p)\n",
        " \n",
        "        # new hidden state:\n",
        "        st = (1-zt)*stm + zt * s_tp\n",
        " \n",
        "        yt = activations.softmax(\n",
        "            K.dot(ytm, self.W_o)\n",
        "            + K.dot(stm, self.U_o)\n",
        "            + K.dot(context, self.C_o)\n",
        "            + self.b_o)\n",
        " \n",
        "        if self.return_probabilities:\n",
        "            return at, [yt, st]\n",
        "        else:\n",
        "            return yt, [yt, st]\n",
        " \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "            For Keras internal compatability checking\n",
        "        \"\"\"\n",
        "        if self.return_probabilities:\n",
        "            return (None, self.timesteps, self.timesteps)\n",
        "        else:\n",
        "            return (None, self.timesteps, self.output_dim)\n",
        " \n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'units': self.units,\n",
        "            'return_probabilities': self.return_probabilities\n",
        "        }\n",
        "        base_config = super(AttentionDecoder, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV_-E7aAOKwz"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIDNhnttOKzl"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBltTNR4iErS"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKMMC7-8iEvS"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtUc7dZPxAMI",
        "outputId": "45c7d354-60fd-4ae3-f66c-2bd92b4e8628"
      },
      "source": [
        "length = 10 + 1\n",
        "lines = []\n",
        "\n",
        "for i in range(length, len(tokens)):\n",
        "  seq = tokens[i-length:i]\n",
        "  line = ' '.join(seq)\n",
        "  lines.append(line)\n",
        "  if i > 5000:\n",
        "    break\n",
        "\n",
        "print(len(lines))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6gfOaLjqZuaU",
        "outputId": "b60172b1-254c-4da8-f4f7-d914ba98e8a5"
      },
      "source": [
        "lines[2]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'except friday this is from monday to thursday i get up'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnNM9qroyyJ5"
      },
      "source": [
        "## Construction of our LSTM model / Training X and Y\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "McTn9gREyY-P",
        "outputId": "c7b087a7-9dbe-4408-d9b3-0bb8f176cd23"
      },
      "source": [
        "tokens[30]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'to'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeP4la-izKNT"
      },
      "source": [
        "import numpy as np \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzYUYwT6zwzQ"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybuHI89A0PpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff049c5d-c8ac-4351-bd2b-1357fd884cc7"
      },
      "source": [
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:,-1]\n",
        "X[0]\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  9, 123, 121,  49, 120,  47,  16, 119,   3, 117])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UxRoMta1RcS",
        "outputId": "00e7f56b-517a-44b5-d85a-39d6880ef119"
      },
      "source": [
        "X[1]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([123, 121,  49, 120,  47,  16, 119,   3, 117,   1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXp1nRvz12Ff"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rUoqRQi1ZZS",
        "outputId": "e99e2c68-5d75-4593-ef42-0c7a8e448114"
      },
      "source": [
        "y[1]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7P79ceW1m2I",
        "outputId": "84049a15-16f4-48cb-bb41-d6fc76a50d35"
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size )\n",
        "X.shape[1]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUJAkcAz20hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a2baea-5815-47ee-86b0-5164ab6cd980"
      },
      "source": [
        "seq_length = X.shape[1]\n",
        "seq_length\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohv0WKTNH89H"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W17FVAMlH9Am"
      },
      "source": [
        "from keras.layers import Embedding"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HP4X71EW1tr"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQybdIk6AcZ4",
        "outputId": "24c736ab-229b-47ce-9cdd-e3f1af4030e4"
      },
      "source": [
        "pip install attention"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: attention in /usr/local/lib/python3.7/dist-packages (4.0)\n",
            "Requirement already satisfied: tensorflow>=2.1 in /usr/local/lib/python3.7/dist-packages (from attention) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from attention) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.4.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.1.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.32.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (2.4.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.12.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (0.3.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1->attention) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (54.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.28.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->attention) (0.4.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->attention) (3.10.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->attention) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->attention) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->attention) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->attention) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->attention) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DQuwkKLAhT8",
        "outputId": "13f63422-c193-4dcc-a055-584abd1ccd04"
      },
      "source": [
        "pip install keras-self-attention"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.7/dist-packages (0.49.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-self-attention) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LPb2ADkAchW"
      },
      "source": [
        "#variational LSTM dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK7o6QKa3B3T"
      },
      "source": [
        "## LSTM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbzx1RRWjKsc"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9OoU4L5ialN"
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfhKcEdxZDSz",
        "outputId": "f98edadc-e5ce-4342-aae5-e474aebdc642"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 50)            6200      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 10, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 124)               12524     \n",
            "=================================================================\n",
            "Total params: 169,624\n",
            "Trainable params: 169,624\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8peP4AohkTvw"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nPeCet6kTzQ",
        "outputId": "e535456f-83ea-4c12-dc44-79419061e35d"
      },
      "source": [
        "model.fit(X, y, batch_size = 50, epochs = 250)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "6/6 [==============================] - 5s 7ms/step - loss: 4.8184 - accuracy: 0.0064 \n",
            "Epoch 2/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.7974 - accuracy: 0.0436\n",
            "Epoch 3/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.7147 - accuracy: 0.0660\n",
            "Epoch 4/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.5018 - accuracy: 0.0421\n",
            "Epoch 5/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.4043 - accuracy: 0.0819\n",
            "Epoch 6/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.3849 - accuracy: 0.0701\n",
            "Epoch 7/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.3260 - accuracy: 0.0832\n",
            "Epoch 8/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.2223 - accuracy: 0.0746\n",
            "Epoch 9/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.2645 - accuracy: 0.0830\n",
            "Epoch 10/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.1806 - accuracy: 0.0789\n",
            "Epoch 11/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 4.1814 - accuracy: 0.0903\n",
            "Epoch 12/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.1665 - accuracy: 0.0852\n",
            "Epoch 13/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.1229 - accuracy: 0.0892\n",
            "Epoch 14/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 4.1081 - accuracy: 0.0843\n",
            "Epoch 15/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 3.9833 - accuracy: 0.1036\n",
            "Epoch 16/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 3.9157 - accuracy: 0.0961\n",
            "Epoch 17/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.9700 - accuracy: 0.1009\n",
            "Epoch 18/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 3.9080 - accuracy: 0.0909\n",
            "Epoch 19/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.8655 - accuracy: 0.0947\n",
            "Epoch 20/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.8498 - accuracy: 0.0877\n",
            "Epoch 21/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.8352 - accuracy: 0.0846\n",
            "Epoch 22/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.6869 - accuracy: 0.1232\n",
            "Epoch 23/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.7401 - accuracy: 0.0942\n",
            "Epoch 24/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.6493 - accuracy: 0.1066\n",
            "Epoch 25/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.7019 - accuracy: 0.1067\n",
            "Epoch 26/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.7151 - accuracy: 0.0895\n",
            "Epoch 27/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.6147 - accuracy: 0.1298\n",
            "Epoch 28/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.5312 - accuracy: 0.1294\n",
            "Epoch 29/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.5507 - accuracy: 0.0961\n",
            "Epoch 30/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.4962 - accuracy: 0.1023\n",
            "Epoch 31/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.5288 - accuracy: 0.1172\n",
            "Epoch 32/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.4867 - accuracy: 0.1038\n",
            "Epoch 33/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.4433 - accuracy: 0.1507\n",
            "Epoch 34/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.4075 - accuracy: 0.1353\n",
            "Epoch 35/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.2912 - accuracy: 0.1428\n",
            "Epoch 36/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.3112 - accuracy: 0.1441\n",
            "Epoch 37/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.2845 - accuracy: 0.1788\n",
            "Epoch 38/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.2894 - accuracy: 0.1370\n",
            "Epoch 39/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.2443 - accuracy: 0.1657\n",
            "Epoch 40/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 3.2018 - accuracy: 0.1722\n",
            "Epoch 41/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.0918 - accuracy: 0.1841\n",
            "Epoch 42/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.0660 - accuracy: 0.1639\n",
            "Epoch 43/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3.0866 - accuracy: 0.1793\n",
            "Epoch 44/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.9837 - accuracy: 0.1986\n",
            "Epoch 45/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.9998 - accuracy: 0.1629\n",
            "Epoch 46/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.8509 - accuracy: 0.1854\n",
            "Epoch 47/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.9175 - accuracy: 0.1921\n",
            "Epoch 48/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.8286 - accuracy: 0.1731\n",
            "Epoch 49/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.7755 - accuracy: 0.2260\n",
            "Epoch 50/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.7593 - accuracy: 0.1930\n",
            "Epoch 51/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.6756 - accuracy: 0.2470\n",
            "Epoch 52/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.6030 - accuracy: 0.2587\n",
            "Epoch 53/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.5850 - accuracy: 0.2170\n",
            "Epoch 54/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.4445 - accuracy: 0.2944\n",
            "Epoch 55/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.4738 - accuracy: 0.2767\n",
            "Epoch 56/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.4092 - accuracy: 0.2739\n",
            "Epoch 57/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.3245 - accuracy: 0.2817\n",
            "Epoch 58/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.2691 - accuracy: 0.3141\n",
            "Epoch 59/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.2335 - accuracy: 0.3365\n",
            "Epoch 60/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.1737 - accuracy: 0.3441\n",
            "Epoch 61/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.1203 - accuracy: 0.3720\n",
            "Epoch 62/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.0185 - accuracy: 0.4213\n",
            "Epoch 63/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 2.0499 - accuracy: 0.4104\n",
            "Epoch 64/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 1.9450 - accuracy: 0.4102\n",
            "Epoch 65/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.8727 - accuracy: 0.4571\n",
            "Epoch 66/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.9000 - accuracy: 0.4739\n",
            "Epoch 67/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.7813 - accuracy: 0.4970\n",
            "Epoch 68/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.8062 - accuracy: 0.4742\n",
            "Epoch 69/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.8234 - accuracy: 0.4371\n",
            "Epoch 70/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.8774 - accuracy: 0.4717\n",
            "Epoch 71/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.7559 - accuracy: 0.4741\n",
            "Epoch 72/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.6286 - accuracy: 0.5411\n",
            "Epoch 73/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.6792 - accuracy: 0.5362\n",
            "Epoch 74/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.5198 - accuracy: 0.5645\n",
            "Epoch 75/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.5128 - accuracy: 0.5814\n",
            "Epoch 76/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.4863 - accuracy: 0.5939\n",
            "Epoch 77/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.4796 - accuracy: 0.5918\n",
            "Epoch 78/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.4037 - accuracy: 0.6443\n",
            "Epoch 79/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.3291 - accuracy: 0.6535\n",
            "Epoch 80/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.3104 - accuracy: 0.6344\n",
            "Epoch 81/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.2764 - accuracy: 0.6842\n",
            "Epoch 82/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.2013 - accuracy: 0.6928\n",
            "Epoch 83/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.2531 - accuracy: 0.6869\n",
            "Epoch 84/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.1244 - accuracy: 0.7090\n",
            "Epoch 85/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.1862 - accuracy: 0.6849\n",
            "Epoch 86/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.0753 - accuracy: 0.7440\n",
            "Epoch 87/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 1.0726 - accuracy: 0.7377\n",
            "Epoch 88/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.0720 - accuracy: 0.7299\n",
            "Epoch 89/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.9615 - accuracy: 0.7943\n",
            "Epoch 90/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.0196 - accuracy: 0.7522\n",
            "Epoch 91/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.9874 - accuracy: 0.7666\n",
            "Epoch 92/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.0829 - accuracy: 0.7267\n",
            "Epoch 93/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.0699 - accuracy: 0.7086\n",
            "Epoch 94/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.9588 - accuracy: 0.7670\n",
            "Epoch 95/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.9233 - accuracy: 0.7771\n",
            "Epoch 96/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8848 - accuracy: 0.7838\n",
            "Epoch 97/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.9103 - accuracy: 0.8155\n",
            "Epoch 98/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.8436 - accuracy: 0.8140\n",
            "Epoch 99/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.8569 - accuracy: 0.7717\n",
            "Epoch 100/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8630 - accuracy: 0.7902\n",
            "Epoch 101/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.7717 - accuracy: 0.8139\n",
            "Epoch 102/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.7912 - accuracy: 0.8143\n",
            "Epoch 103/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.7700 - accuracy: 0.8316\n",
            "Epoch 104/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.7364 - accuracy: 0.8339\n",
            "Epoch 105/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.7405 - accuracy: 0.8258\n",
            "Epoch 106/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.7276 - accuracy: 0.8377\n",
            "Epoch 107/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6779 - accuracy: 0.8596\n",
            "Epoch 108/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6162 - accuracy: 0.8658\n",
            "Epoch 109/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6456 - accuracy: 0.8596\n",
            "Epoch 110/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.6476 - accuracy: 0.8533\n",
            "Epoch 111/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5899 - accuracy: 0.8663\n",
            "Epoch 112/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5723 - accuracy: 0.8859\n",
            "Epoch 113/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5512 - accuracy: 0.8903\n",
            "Epoch 114/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5346 - accuracy: 0.8880\n",
            "Epoch 115/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5584 - accuracy: 0.8624\n",
            "Epoch 116/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.4894 - accuracy: 0.8795\n",
            "Epoch 117/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5537 - accuracy: 0.8869\n",
            "Epoch 118/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5587 - accuracy: 0.8748\n",
            "Epoch 119/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4935 - accuracy: 0.8959\n",
            "Epoch 120/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5257 - accuracy: 0.8905\n",
            "Epoch 121/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4850 - accuracy: 0.9054\n",
            "Epoch 122/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4986 - accuracy: 0.8566\n",
            "Epoch 123/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5580 - accuracy: 0.8525\n",
            "Epoch 124/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.4480 - accuracy: 0.9073\n",
            "Epoch 125/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.4893 - accuracy: 0.8887\n",
            "Epoch 126/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5198 - accuracy: 0.8639\n",
            "Epoch 127/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5932 - accuracy: 0.8421\n",
            "Epoch 128/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5380 - accuracy: 0.8491\n",
            "Epoch 129/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.6245 - accuracy: 0.8413\n",
            "Epoch 130/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5993 - accuracy: 0.8297\n",
            "Epoch 131/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.5992 - accuracy: 0.8421\n",
            "Epoch 132/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.6155 - accuracy: 0.8470\n",
            "Epoch 133/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4951 - accuracy: 0.8637\n",
            "Epoch 134/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5169 - accuracy: 0.8907\n",
            "Epoch 135/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.4637 - accuracy: 0.8922\n",
            "Epoch 136/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4515 - accuracy: 0.9045\n",
            "Epoch 137/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4640 - accuracy: 0.8967\n",
            "Epoch 138/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5707 - accuracy: 0.8365\n",
            "Epoch 139/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5822 - accuracy: 0.8384\n",
            "Epoch 140/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.4637 - accuracy: 0.8918\n",
            "Epoch 141/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.4926 - accuracy: 0.8644\n",
            "Epoch 142/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3703 - accuracy: 0.9264\n",
            "Epoch 143/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3682 - accuracy: 0.9293\n",
            "Epoch 144/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3172 - accuracy: 0.9364\n",
            "Epoch 145/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3219 - accuracy: 0.9331\n",
            "Epoch 146/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2818 - accuracy: 0.9536\n",
            "Epoch 147/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.3051 - accuracy: 0.9347\n",
            "Epoch 148/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2725 - accuracy: 0.9539\n",
            "Epoch 149/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2588 - accuracy: 0.9693\n",
            "Epoch 150/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2991 - accuracy: 0.9440\n",
            "Epoch 151/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2564 - accuracy: 0.9487\n",
            "Epoch 152/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2406 - accuracy: 0.9712\n",
            "Epoch 153/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2472 - accuracy: 0.9704\n",
            "Epoch 154/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2186 - accuracy: 0.9737\n",
            "Epoch 155/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2131 - accuracy: 0.9723\n",
            "Epoch 156/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2201 - accuracy: 0.9519\n",
            "Epoch 157/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2262 - accuracy: 0.9674\n",
            "Epoch 158/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2315 - accuracy: 0.9555\n",
            "Epoch 159/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2195 - accuracy: 0.9657\n",
            "Epoch 160/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1823 - accuracy: 0.9679\n",
            "Epoch 161/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1796 - accuracy: 0.9676\n",
            "Epoch 162/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2035 - accuracy: 0.9630\n",
            "Epoch 163/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1862 - accuracy: 0.9590\n",
            "Epoch 164/250\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1602 - accuracy: 0.9802\n",
            "Epoch 165/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1600 - accuracy: 0.9798\n",
            "Epoch 166/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1501 - accuracy: 0.9869\n",
            "Epoch 167/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1459 - accuracy: 0.9834\n",
            "Epoch 168/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1754 - accuracy: 0.9684\n",
            "Epoch 169/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1656 - accuracy: 0.9807\n",
            "Epoch 170/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1518 - accuracy: 0.9850\n",
            "Epoch 171/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1532 - accuracy: 0.9845\n",
            "Epoch 172/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1421 - accuracy: 0.9893\n",
            "Epoch 173/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1516 - accuracy: 0.9691\n",
            "Epoch 174/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1429 - accuracy: 0.9812\n",
            "Epoch 175/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1312 - accuracy: 0.9780\n",
            "Epoch 176/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1321 - accuracy: 0.9870\n",
            "Epoch 177/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1429 - accuracy: 0.9776\n",
            "Epoch 178/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1318 - accuracy: 0.9798\n",
            "Epoch 179/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1714 - accuracy: 0.9748\n",
            "Epoch 180/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1895 - accuracy: 0.9446\n",
            "Epoch 181/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2120 - accuracy: 0.9428\n",
            "Epoch 182/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1648 - accuracy: 0.9743\n",
            "Epoch 183/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.9668\n",
            "Epoch 184/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1709 - accuracy: 0.9672\n",
            "Epoch 185/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.2111 - accuracy: 0.9542\n",
            "Epoch 186/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1700 - accuracy: 0.9749\n",
            "Epoch 187/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1592 - accuracy: 0.9769\n",
            "Epoch 188/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1243 - accuracy: 0.9788\n",
            "Epoch 189/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1269 - accuracy: 0.9857\n",
            "Epoch 190/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1217 - accuracy: 0.9854\n",
            "Epoch 191/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1425 - accuracy: 0.9684\n",
            "Epoch 192/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1136 - accuracy: 0.9895\n",
            "Epoch 193/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1106 - accuracy: 0.9867\n",
            "Epoch 194/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1037 - accuracy: 0.9877\n",
            "Epoch 195/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1113 - accuracy: 0.9883\n",
            "Epoch 196/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0970 - accuracy: 0.9876\n",
            "Epoch 197/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1160 - accuracy: 0.9740\n",
            "Epoch 198/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0993 - accuracy: 0.9863\n",
            "Epoch 199/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0842 - accuracy: 0.9860\n",
            "Epoch 200/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1147 - accuracy: 0.9698\n",
            "Epoch 201/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0861 - accuracy: 0.9872\n",
            "Epoch 202/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0884 - accuracy: 0.9839\n",
            "Epoch 203/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0897 - accuracy: 0.9877\n",
            "Epoch 204/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0869 - accuracy: 0.9889\n",
            "Epoch 205/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0722 - accuracy: 0.9883\n",
            "Epoch 206/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0748 - accuracy: 0.9904\n",
            "Epoch 207/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0669 - accuracy: 0.9935\n",
            "Epoch 208/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0756 - accuracy: 0.9844\n",
            "Epoch 209/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0781 - accuracy: 0.9862\n",
            "Epoch 210/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0889 - accuracy: 0.9775\n",
            "Epoch 211/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0721 - accuracy: 0.9879\n",
            "Epoch 212/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0729 - accuracy: 0.9852\n",
            "Epoch 213/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0740 - accuracy: 0.9944\n",
            "Epoch 214/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0813 - accuracy: 0.9810\n",
            "Epoch 215/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0796 - accuracy: 0.9839\n",
            "Epoch 216/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0818 - accuracy: 0.9763\n",
            "Epoch 217/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0721 - accuracy: 0.9898\n",
            "Epoch 218/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0732 - accuracy: 0.9910\n",
            "Epoch 219/250\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0634 - accuracy: 0.9952\n",
            "Epoch 220/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0632 - accuracy: 0.9921\n",
            "Epoch 221/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0694 - accuracy: 0.9929\n",
            "Epoch 222/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0620 - accuracy: 0.9868\n",
            "Epoch 223/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0598 - accuracy: 0.9935\n",
            "Epoch 224/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0660 - accuracy: 0.9864\n",
            "Epoch 225/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0636 - accuracy: 0.9891\n",
            "Epoch 226/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0585 - accuracy: 0.9941\n",
            "Epoch 227/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0640 - accuracy: 0.9831\n",
            "Epoch 228/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0587 - accuracy: 0.9862\n",
            "Epoch 229/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0531 - accuracy: 0.9910\n",
            "Epoch 230/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9893\n",
            "Epoch 231/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0572 - accuracy: 0.9826\n",
            "Epoch 232/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0567 - accuracy: 0.9862\n",
            "Epoch 233/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0491 - accuracy: 0.9912\n",
            "Epoch 234/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0476 - accuracy: 0.9944\n",
            "Epoch 235/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0574 - accuracy: 0.9909\n",
            "Epoch 236/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0512 - accuracy: 0.9921\n",
            "Epoch 237/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0581 - accuracy: 0.9855\n",
            "Epoch 238/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0579 - accuracy: 0.9909\n",
            "Epoch 239/250\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0489 - accuracy: 0.9930\n",
            "Epoch 240/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0493 - accuracy: 0.9961\n",
            "Epoch 241/250\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0600 - accuracy: 0.9768\n",
            "Epoch 242/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 0.9945\n",
            "Epoch 243/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0513 - accuracy: 0.9921\n",
            "Epoch 244/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0475 - accuracy: 0.9874\n",
            "Epoch 245/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0466 - accuracy: 0.9902\n",
            "Epoch 246/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0396 - accuracy: 0.9943\n",
            "Epoch 247/250\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0436 - accuracy: 0.9937\n",
            "Epoch 248/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0414 - accuracy: 0.9953\n",
            "Epoch 249/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0466 - accuracy: 0.9899\n",
            "Epoch 250/250\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0387 - accuracy: 0.9902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f34d028dfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0RinKKVkT2Q"
      },
      "source": [
        ""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wcDffJ3MlswC",
        "outputId": "27764cdc-228b-4e48-f562-ac8ec4b9bdaa"
      },
      "source": [
        "seed_text=lines[150]\n",
        "seed_text"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have lunch and at four i go to my course im'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbZ_Cg_1lszx"
      },
      "source": [
        ""
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxQNVl-NKElr"
      },
      "source": [
        "\n",
        "import sys, os, re, csv, codecs, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2JECigBSll3",
        "outputId": "2267f96e-f390-4baf-c04a-434608028e97"
      },
      "source": [
        "\n",
        "batch_size = 64\n",
        "epochs = 2\n",
        "model.fit(X,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "4/4 [==============================] - 1s 313ms/step - loss: 0.0373 - accuracy: 0.9960 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.0365 - accuracy: 0.9960 - val_loss: 0.0590 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f33ed42f1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "Lcp3-fbzKHJD",
        "outputId": "42463d25-a88b-4ef2-c9ae-28c60f622720"
      },
      "source": [
        "\n",
        "history = model.history\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "# Get number of epochs\\n\",\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\\n\",\n",
        "plt.plot(epochs, acc)\n",
        "plt.plot(epochs, val_acc)\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure() \n",
        "\n",
        "# Plot training and validation loss per epoch\\n\",\n",
        "plt.plot(epochs, loss)\n",
        "plt.plot(epochs, val_loss)\n",
        "plt.title('Training and validation loss')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training and validation loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD2wqV2sNCBm",
        "outputId": "be41fbb7-3152-44cd-b5e3-5b9fff647c7b"
      },
      "source": [
        "model.evaluate(X, y, verbose=1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0379 - accuracy: 0.9964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03785165771842003, 0.9964413046836853]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q3gEz80lXltJ",
        "outputId": "2c7af5df-c208-4614-deb2-9ba695e44408"
      },
      "source": [
        "seed_text=lines[0]\n",
        "seed_text"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'on weekdays except friday this is from monday to thursday i'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgqu8cYpO--z"
      },
      "source": [
        "\n",
        "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
        "  text = []\n",
        "\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
        "\n",
        "    y_predict = model.predict_classes(encoded)\n",
        "\n",
        "    predicted_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == y_predict:\n",
        "        predicted_word = word\n",
        "        break\n",
        "    seed_text = seed_text + ' ' + predicted_word\n",
        "    text.append(predicted_word)\n",
        "  return ' '.join(text)\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "3uXN43vqNUHs",
        "outputId": "b4e711a4-c94b-4485-dde8-df128c7482f2"
      },
      "source": [
        "\n",
        "generate_text_seq(model, tokenizer, seq_length, seed_text, 5) \n",
        "\n",
        " \n",
        " "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'get up early at half'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "3jd-CagnQkMA",
        "outputId": "8cea0741-7832-40bc-8617-aa9da5559e48"
      },
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "def prediction_text(seed_text,n_words):    \n",
        " text_seq_length = 3\n",
        " # seed_text =             \"this is the\" ...\n",
        " output = generate_text_seq(model, tokenizer, text_seq_length, seed_text, int(n_words)) \n",
        " return output\n",
        "\n",
        "\n",
        "iface = gr.Interface(fn=prediction_text, inputs= [\"text\", \"text\"], outputs=\"text\")\n",
        "iface.launch()\n",
        "\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "This share link will expire in 24 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted (NEW!)\n",
            "Running on External URL: https://24076.gradio.app\n",
            "Interface loading below...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1000\"\n",
              "            height=\"500\"\n",
              "            src=\"https://24076.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f34e07636d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Flask 'gradio.networking'>,\n",
              " 'http://127.0.0.1:7860/',\n",
              " 'https://24076.gradio.app')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    }
  ]
}